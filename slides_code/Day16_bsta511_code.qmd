---
title: "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)"
subtitle: "BSTA 511/611"
author: "Meike Niederhausen, PhD"
institute: "OHSU-PSU School of Public Health"
date: "12/2/2024"
categories: ["Week 10"]
format: 
  html:
    link-external-newwindow: true
    toc: true
    source: repo
    html-math-method: mathjax
execute:
  echo: true
  freeze: auto  # re-render only when source changes
# editor: visual
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: "setup"
#| include: false
knitr::opts_chunk$set(echo = TRUE, fig.height=3, fig.width=5, message = F)
```

# Load packages

* Packages need to be loaded _every time_ you restart R or render an Qmd file

```{r}
# run these every time you open Rstudio
library(tidyverse)    
library(oibiostat)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) 
library(here) 
library(pwr) 
library(gridExtra) # NEW!!!

```


- You can check whether a package has been loaded or not 
  - by looking at the Packages tab and 
  - seeing whether it has been checked off or not

# Goals for today (Sections 6.3-6.4)

Simple Linear Regression Part 2 

+ Review of 
    * best-fit line (aka regression line or least-squares line)
    * residuals
    * population model
+ LINE conditions and how to assess them
    * New diagnostic tools:
        * Normal QQ plots of residuals
        * Residual plots
+ Coefficient of determination ($R^2$)
+ Regression inference
    1. Inference for population [__slope__]{style="color:green"} $\beta_1$
        * CI & hypothesis test
    2. CI for mean response $\mu_{Y|x^*}$
    3. Prediction interval for predicting __individual__ observations
    + Confidence bands vs. predictions bands


# Life expectancy vs. female adult literacy rate

<https://www.gapminder.org/tools/#$model$markers$bubble$encoding$x$data$concept=literacy_rate_adult_female_percent_of_females_ages_15_above&source=sg&space@=country&=time;;&scale$domain:null&zoomed:null&type:null;;&frame$value=2011;;;;;&chart-type=bubbles&url=v1>


## Dataset description 

* Data file: `lifeexp_femlit_water_2011.csv`
* Data were downloaded from <https://www.gapminder.org/data/>
* 2011 is the most recent year with the most complete data
* __Life expectancy__ = the average number of years a newborn child would live if current mortality patterns were to stay the same. Source: <https://www.gapminder.org/data/documentation/gd004/>

* __Adult literacy rate__ is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life. Source: <http://data.uis.unesco.org/>

* __At least basic water source (%)__ = the percentage of people using at least basic water services. This indicator encompasses both people using basic water services as well as those using safely managed water services. Basic drinking water services is defined as drinking water from an improved source, provided collection time is not more than 30 minutes for a round trip. Improved water sources include piped water, boreholes or tubewells, protect dug wells, protected springs, and packaged or delivered water.


## Get to know the data

Load data

```{r}
gapm_original <- read_csv(here::here("data", "lifeexp_femlit_water_2011.csv"))
```

Glimpse of the data

```{r}
glimpse(gapm_original)
```

Note the missing values for our variables of interest

```{r}
gapm_original %>% select(life_expectancy_years_2011, female_literacy_rate_2011) %>% 
  get_summary_stats()
```



## Remove missing values

Remove rows with missing data for life expectancy and female literacy rate

```{r}
gapm <- gapm_original %>% 
  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)

glimpse(gapm)

```

No missing values now for our variables of interest

```{r}
gapm %>% select(life_expectancy_years_2011, female_literacy_rate_2011) %>% 
  get_summary_stats()
```


:::{.callout-important}
* Removing the rows with missing data was not needed to run the regression model.
* I did this step since later we will be calculating the standard deviations of the explanatory and response variables for _just the values included in the regression model_. It'll be easier to do this if we remove the missing values now. 
:::



# Regression line = best-fit line

$$\widehat{y} = b_0 + b_1 \cdot x $$

* $\hat{y}$ is the predicted outcome for a specific value of $x$.
* $b_0$ is the intercept
* $b_1$ is the slope of the line, i.e., the increase in $\hat{y}$ for every increase of one (unit increase) in $x$.
    - slope = *rise over run*


```{r}
#| echo: false
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point() +
  labs(x = "female literacy rate", 
       y = "life expectancy",
       title = "Life expectancy vs. female literacy rate") +  
  geom_smooth(method = "lm", se = FALSE)
```

* __Intercept__
    - The expected outcome for the $y$-variable when the $x$-variable is 0.
* __Slope__
    - For every increase of 1 unit in the $x$-variable, there is an expected increase of, on average, $b_1$ units in the $y$-variable.
    - We only say that there is an expected increase and not necessarily a causal increase.



## Regression in R: `lm()`, `summary()`, & `tidy()`

```{r}
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)
summary(model1)

tidy(model1) %>% gt()
```

Regression equation for our model:

$$\widehat{\textrm{life expectancy}} = 50.9 + 0.232 \cdot \textrm{female literacy rate} $$



## Residuals

* __Observed values__ $y_i$ 
  * the values in the dataset

* __Fitted values__ $\widehat{y}_i$ 
  * the values that fall on the best-fit line for a specific $x_i$

* __Residuals__ $e_i = y_i - \widehat{y}_i$ 
  * the differences between the observed and fitted values

```{r}
#| echo: false
# code from https://drsimonj.svbtle.com/visualising-residuals

model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)
regression_points <- augment(model1)

ggplot(regression_points, 
       aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(
    xend = female_literacy_rate_2011, 
    yend = .fitted), 
    alpha = .2) +
  # > Color adjustments made here...
  geom_point(aes(color = .resid), size = 2) +  # Color mapped here
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +  # Colors to use here
  guides(color = "none") +
  geom_point(aes(y = .fitted), shape = 1, size = 2) +
  labs(x = "Female literacy rate", 
       y = "Average life expectancy",
       title = "Regression line with residuals") +
  theme_bw() 
```


## The (population) regresison model

* The (population) regression model is denoted by

$$Y = \beta_0 + \beta_1 \cdot X + \epsilon$$

* $\beta_0$ and $\beta_1$ are unknown population parameters
* $\epsilon$ (epsilon) is the error about the line
    * It is assumed to be a random variable: 
        * $\epsilon \sim N(0, \sigma^2)$
        * variance $\sigma^2$ is constant

_See slides for image._


* The [__line__]{style="color:blue"} is the average (expected) value of $Y$ given a value of $x$: [$E(Y|x)$]{style="color:blue"}.

* The point estimates for $\beta_0$ and $\beta_1$ based on a sample are denoted by $b_0, b_1, s_{residuals}^2$
    * Note: also common notation is $\widehat{\beta}_0, \widehat{\beta}_1, \widehat{\sigma}^2$



# What are the LINE conditions?

For "good" model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.  

Briefly: 

* __L__ inearity of relationship between variables
* __I__ ndependence of the Y values
* __N__ ormality of the residuals
* __E__ quality of variance of the residuals (homoscedasticity)

[More in depth](https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions):

* __L__ : there is a linear relationship between the mean response (Y) and the explanatory variable (X),
* __I__ : the errors are independent—there’s no connection between how far any two points lie from the regression line,
* __N__ : the responses are normally distributed at each level of X, and
* __E__ : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X.



## L: Linearity of relationship between variables

Is the association between the variables linear?

* Diagnostic tools:
    * Scatterplot
    * Residual plot (see later section for E : Equality of variance of the residuals)

```{r}
#| echo: false
#| fig.width: 12.0
#| fig.height: 7.0
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point() +
  labs(x = "female literacy rate", 
       y = "life expectancy",
       title = "Life expectancy vs. female literacy rate in 2011") +  
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(se = FALSE, color = "black")
```



## I: Independence of the residuals ($Y$ values)

* __Are the data points independent of each other?__

* Examples of when they are *not* independent, include
    * repeated measures (such as baseline, 3 months, 6 months)
    * data from clusters, such as different hospitals or families

* This condition is checked by reviewing the study *design* and not by inspecting the data

* How to analyze data using regression models when the $Y$-values are not independent is covered in BSTA 519 (Longitudinal data) 



# N: Normality of the residuals

* Extract residuals from regression model in R
* Diagnostic tools:
    * Distribution plots of residuals
    * QQ plots



## N: Normality of the residuals

* The responses Y are normally distributed at each level of x

_See slides for image._



## Extract model's residuals in R

* First extract the residuals' values from the model output using the `augment()` function from the `broom` package.
* Get a tibble with the orginal data, as well as the residuals and some other important values.

```{r}
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, 
                data = gapm)
aug1 <- augment(model1) 

glimpse(aug1)
```



## Check normality with "usual" distribution plots

Note that below I save each figure, and then combine them together in one row of output using `grid.arrange()` from the `gridExtra` package.

```{r}
#| fig.height: 3.0
#| fig.width: 6.0
hist1 <- ggplot(aug1, aes(x = .resid)) +
  geom_histogram()

density1 <- ggplot(aug1, aes(x = .resid)) +
  geom_density()

box1 <- ggplot(aug1, aes(x = .resid)) +
  geom_boxplot()

library(gridExtra) # NEW!!!
grid.arrange(hist1, density1, box1, nrow = 1)
```



## Normal QQ plots (QQ = quantile-quantile)

* It can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not
* QQ plots are often used to help with this


* _Vertical axis_: __data quantiles__
  * data points are sorted in order and 
  * assigned quantiles based on how many data points there are
* _Horizontal axis_: __theoretical quantiles__
    * mean and standard deviation (SD) calculated from the data points
    * theoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data


```{r}
#| echo: false
ggplot(aug1, aes(sample = .resid)) + 
  stat_qq() +     # points
  stat_qq_line()  # line
```

* __Data are approximately normal if points fall on a line.__

See more info at <https://data.library.virginia.edu/understanding-QQ-plots/> 



### Examples of Normal QQ plots 

_See slides for examples._



## QQ plot of residuals of `model1`

```{r}
grid.arrange(hist1, density1, box1, nrow = 1)
```

```{r}
ggplot(aug1, aes(sample = .resid)) + 
  stat_qq() +     # points
  stat_qq_line()  # line
```



## Compare to randomly generated Normal QQ plots 


How "_good_" we can expect a QQ plot to look depends on the sample size. 

* The QQ plots on the next slides are randomly generated
    * using random samples from actual standard normal distributions $N(0,1)$.

* Thus, all the points in the QQ plots __should theoretically__ fall in a line

* However, there is sampling variability...


### Randomly generated Normal QQ plots: n=100 

* Note that `stat_qq_line()` doesn't work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is $y=x$ in this case.)


```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 100

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```



### Examples of simulated Normal QQ plots: n=10 

With fewer data points,

* simulated QQ plots are more likely to look "less normal" 
* even though the data points were sampled from normal distributions.


```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 10  # only change made to code!

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```



### Examples of simulated Normal QQ plots: n=1,000 

With more data points,

* simulated QQ plots are more likely to look "more normal" 

```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 1000 # only change made to code!

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```



## Back to our example

Residuals from Life Expectancy vs. Female Literacy Rate Regression

```{r}
ggplot(aug1, 
      aes(sample = .resid)) + 
  stat_qq() + 
  stat_qq_line() 
```

Simulated QQ plot of Normal Residuals with n = 80  

```{r}
# number of observations 
# in fitted model
nobs(model1) 

ggplot() +
  stat_qq(aes(
    sample = rnorm(80))) + 
  geom_abline(
    intercept = 0, slope = 1, 
    color = "blue")
```



# E: Equality of variance of the residuals {.nostretch}

* Homoscedasticity
* Diagnostic tool: __residual plot__



## Residual plot

* $x$ = explanatory variable from regression model  
    * (or the fitted values for a multiple regression)
* $y$ = residuals from regression model

```{r}
names(aug1)

ggplot(aug1, 
       aes(x = female_literacy_rate_2011, 
           y = .resid)) + 
  geom_point() +
  geom_abline(
    intercept = 0, 
    slope = 0, 
    color = "orange") +
  labs(title = "Residual plot")
```



## E: Equality of variance of the residuals (Homoscedasticity)

* The __variance__ or, equivalently, the standard deviation of the responses is __equal for all values of x__.
* This is called __homoskedasticity__ (top row)
* If there is __heteroskedasticity__ (bottom row), then the assumption is not met.

_See slides for image._



# $R^2$ = Coefficient of determination

Another way to assess model fit



## $R^2$ = Coefficient of determination (1/2)

+ Recall that the correlation coefficient $r$ measures the strength of the linear relationship between two numerical variables
+ $R^2$ is usually used to measure the strength of a _linear fit_
  + For a simple linear regression model (one numerical predictor), $R^2$ is just the square of the correlation coefficient
+ In general, $R^2$ is the proportion of the variability of the dependent variable that is __explained__ by the independent variable(s)

$$R^2 = \frac{\textrm{variance of predicted y-values}}
{\textrm{variance of observed y-values}} = \frac{\sum_{i=1}^n(\widehat{y}_i-\bar{y})^2}
{\sum_{i=1}^n(y_i-\bar{y})^2} 
 = \frac{s_y^2 - s_{\textrm{residuals}}^2}
{s_y^2}$$
$$R^2 = 1- \frac{s_{\textrm{residuals}}^2}
{s_y^2}$$
where $\frac{s_{\textrm{residuals}}^2}{s_y^2}$ is the proportion of "unexplained" variability in the $y$ values,  
and thus $R^2 = 1- \frac{s_{\textrm{residuls}}^2}{s_y^2}$ is the proportion of "explained" variability in the $y$ values 



## $R^2$ = Coefficient of determination (2/2)

+ Recall, $-1<r<1$

+ Thus, $0<R^2<1$

+ In practice, we want "high" $R^2$ values, i.e. $R^2$ as close to 1 as possible.

Calculating $R^2$ in R using `glance()` from the `broom` package:
```{r}
glance(model1)
glance(model1)$r.squared
```

:::{.callout-warning}
* A model can have a high $R^2$ value when there is a curved pattern.
* Always first check whether a linear model is reasonable or not.
:::



## $R^2$ in `summary()` R output

```{r}
summary(model1)
```

Compare to the square of the correlation coefficient $r$:

```{r}
r <- cor(x = gapm$life_expectancy_years_2011, 
    y = gapm$female_literacy_rate_2011,
    use =  "complete.obs")
r
r^2
```


# Regression inference

1. Inference for population [__slope__]{style="color:green"} $\beta_1$
2. CI for mean response $\mu_{Y|x^*}$
3. Prediction interval for predicting __individual__ observations


## Inference for population [__slope__]{style="color:green"} $\beta_1$


```{r}
# Fit regression model:
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)
# Get regression table:
tidy(model1, conf.int = TRUE) %>% gt() # conf.int = TRUE part is new! 
```


\begin{align}
\widehat{y} =& b_0 + b_1 \cdot x\\
\widehat{\text{life expectancy}} =& 50.9 + 0.232 \cdot \text{female literacy rate}
\end{align}

+ What are $H_0$ and $H_A$?
+ How do we calculate the standard error, statistic, _p_-value, and CI?

:::{.callout-note}
* We can also test & calculate CI for the population intercept
* This will be covered in BSTA 512
:::



### Inference for the population [__slope__]{style="color:green"}: CI and hypothesis test

__Population model__  
_line + random "noise"_

$$Y = \beta_0 + \beta_1 \cdot X + \varepsilon$$
with $\varepsilon \sim N(0,\sigma)$  
$\sigma$ is the variability (SD) of the residuals

<br>

__Sample best-fit (least-squares) line:__

$$\widehat{y} = b_0 + b_1 \cdot x $$

Note: Some sources use $\widehat{\beta}$ instead of $b$. 


+ Construct a __95% confidence interval__ for the __population slope__ $\beta_1$

<br>

+ Conduct the __hypothesis test__ 

\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}

<br>

_Note: R reports p-values for 2-sided tests_



### CI for population [__slope__]{style="color:green"} $\beta_1$

Recall the general CI formula: 

$$\textrm{Point Estimate} \pm t^*\cdot SE_{\textrm{Point Estimate}}$$

For the CI of the coefficient $b_1$ this translates to

$$b_1 \pm t^*\cdot SE_{b_1}$$
where $t^*$ is the critical value from a $t$-distribution with $df = n -2$.

<br>

_How is_ $\text{SE}_{b_1}$ _calculated?_ See next slide.

```{r}
tidy(model1, conf.int = TRUE)
```



### Standard error of fitted slope $b_1$

$$\text{SE}_{b_1} = \frac{s_{\textrm{residuals}}}{s_x\sqrt{n-1}}$$

$\text{SE}_{b_1}$ is the __variability__ of the statistic $b_1$

+ $s_{\textrm{residuals}}$ is the sd of the residuals
+ $s_x$ is the sample sd of the explanatory variable $x$
+ $n$ is the sample size, or the number of (complete) pairs of points

```{r}
glance(model1)

# standard deviation of the residuals (Residual standard error in summary() output)
(s_resid <- glance(model1)$sigma)

# standard deviation of x's
(s_x <- sd(gapm$female_literacy_rate_2011))

# number of pairs of complete observations
(n <- nobs(model1))

(se_b1 <- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output
```



### Calculate CI for population [__slope__]{style="color:green"} $\beta_1$

$$b_1 \pm t^*\cdot SE_{b_1}$$  

where $t^*$ is the $t$-distribution critical value with $df = n -2$.

```{r}
tidy(model1, conf.int = TRUE) %>% gt()
```

Save regression output for the row with the slope's information: 

```{r}
model1_b1 <-tidy(model1) %>% filter(term == "female_literacy_rate_2011")
model1_b1 %>% gt()
```

Save values needed for CI:

```{r}
b1 <- model1_b1$estimate
SE_b1 <- model1_b1$std.error

nobs(model1) # sample size n
(tstar <- qt(.975, df = 80-2))
```

Compare CI bounds below with the ones in the regression table above.

```{r}
(CI_LB <- b1 - tstar*SE_b1)
(CI_UB <- b1 + tstar*SE_b1)
```



### Hypothesis test for population [__slope__]{style="color:green"} $\beta_1$

\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}

The __test statistic__ for $b_1$ is 

$$t = \frac{ b_1 - \beta_1}{ \text{SE}_{b_1}} = \frac{ b_1}{ \text{SE}_{b_1}}$$

when we assume $H_0: \beta_1 = 0$ is true.

```{r}
tidy(model1, conf.int = TRUE) %>% gt()
```

Calculate the test statistic using the values in the regression table:

```{r}
# recall model1_b1 is regression table restricted to b1 row
(TestStat <- model1_b1$estimate / model1_b1$std.error)
```

Compare this test statistic value to the one from the regression table above



### $p$-value for testing population [__slope__]{style="color:green"} $\beta_1$

* As usual, the $p$-value is the _probability of obtaining a test statistic_ __just as extreme or more extreme__ _than the observed test statistic assuming the null hypothesis_ $H_0$ _is true._

* To calculate the $p$-value, we need to know the probability distribution of the test statistic (the _null distribution_) assuming $H_0$ is true.

+ Statistical theory tells us that the test statistic $t$ can be modeled by a [__$t$-distribution__]{style="color:green"} with [__$df = n-2$__]{style="color:green"}.

+ Recall that this is a 2-sided test:

```{r}
(pv = 2*pt(TestStat, df=80-2, lower.tail=F))
```

Compare the $p$-value to the one from the regression table below

```{r}
tidy(model1, conf.int = TRUE) %>% gt()  # compare p-value calculated above to p-value in table
```



# Prediction (& inference)

1. Prediction for mean response 
2. Prediction for new individual observation



## Prediction with regression line

```{r}
#| echo: false
tidy(model1) %>% gt()
```

$$\widehat{\textrm{life expectancy}} = 50.9 + 0.232 \cdot \textrm{female literacy rate} $$

What is the predicted life expectancy for a country with female literacy rate 60%?

$$\widehat{\textrm{life expectancy}} = 50.9 + 0.232 \cdot 60 = `r 50.9 + 0.232*60`$$

```{r}
(y_60 <- 50.9 + 0.232*60)
```

<br>

* How do we interpret the predicted value?
* How variable is it?



### Prediction with regression line

Recall the population model:

_line + random "noise"_

$$Y = \beta_0 + \beta_1 \cdot X + \varepsilon$$
with $\varepsilon \sim N(0,\sigma)$  
$\sigma$ is the variability (SD) of the residuals

<br>

* When we take the expected value, at a given value $x^*$, we have that the predicted response is the average expected response at $x^*$:

$$\widehat{E[Y|x^*]} = b_0 + b_1 x^*$$

```{r}
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point() +
  labs(x = "female literacy rate", 
       y = "life expectancy",
       title = "Life expectancy vs. female literacy rate") +  
  geom_smooth(method = "lm", se = TRUE) +
  geom_vline(xintercept = 60, color = "green3")
```

* These are the points on the regression line.
* The mean responses has variability, and we can calculate a CI for it, for every value of $x^*$. 



### CI for mean response $\mu_{Y|x^*}$

$$\widehat{E[Y|x^*]} \pm t_{n-2}^* \cdot SE_{\widehat{E[Y|x^*]}}$$

* $SE_{\widehat{E[Y|x^*]}}$ is calculated using

$$SE_{\widehat{E[Y|x^*]}} = s_{residuals} \sqrt{\frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}$$

+ $\widehat{E[Y|x^*]}$ is the predicted value at the specified point $x^*$ of the explanatory variable
+ $s_{\textrm{residuals}}^2$ is the sd of the residuals
+ $n$ is the sample size, or the number of (complete) pairs of points
+ $\bar{x}$ is the sample mean of the explanatory variable $x$
+ $s_x$ is the sample sd of the explanatory variable $x$

<br>

* Recall that $t_{n-2}^*$ is calculated using `qt()` and depends on the confidence level.



### Example: CI for mean response $\mu_{Y|x^*}$

__Find the 95% CI for the mean life expectancy when the female literacy rate is 60.__

\begin{align}
\widehat{E[Y|x^*]} &\pm t_{n-2}^* \cdot SE_{\widehat{E[Y|x^*]}}\\
64.8596 &\pm 1.990847 \cdot s_{residuals} \sqrt{\frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}\\
64.8596 &\pm 1.990847 \cdot 6.142157 \sqrt{\frac{1}{80} + \frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\
64.8596 &\pm 1.990847 \cdot 0.9675541\\
64.8596 &\pm 1.926252\\
(62.93335 &, 66.78586)
\end{align}

```{r}
(Y60 <- 50.9278981 + 0.2321951 * 60)
(tstar <- qt(.975, df = 78))

(s_resid <- glance(model1)$sigma)
(n <- nobs(model1))

(mx <- mean(gapm$female_literacy_rate_2011))
(s_x <- sd(gapm$female_literacy_rate_2011))

(SE_Yx <- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))

(MOE_Yx <- SE_Yx*tstar)

Y60 - MOE_Yx
Y60 + MOE_Yx
```



### Example: Using R for CI for mean response $\mu_{Y|x^*}$

__Find the 95% CI's for the mean life expectancy when the female literacy rate is 40, 60, and 80.__

* Use the base R `predict()` function
* Requires specification of a `newdata` "value"
  * The `newdata` value is $x^*$
  * This has to be in the format of a data frame though
  * with column name identical to the predictor variable in the model 

```{r}
newdata <- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) 
newdata

predict(model1, 
        newdata=newdata, 
        interval="confidence")
```



### Interpretation

We are 95% confident that the __average__ life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years.



### Confidence bands for mean response $\mu_{Y|x^*}$

* Often we plot the CI for many values of X, creating __confidence bands__
* The confidence bands are what ggplot creates when we set `se = TRUE` within `geom_smooth`
* For what values of x are the confidence bands (intervals) narrowest?

```{r}
ggplot(gapm,
       aes(x=female_literacy_rate_2011, 
           y=life_expectancy_years_2011)) +
  geom_point()+
  geom_smooth(method = lm, se=TRUE)+
  ggtitle("Life expectancy vs. female literacy rate") 
```



### Width of confidence bands for mean response $\mu_{Y|x^*}$

* For what values of $x^*$ are the confidence bands (intervals) narrowest? widest?

\begin{align}
\widehat{E[Y|x^*]} &\pm t_{n-2}^* \cdot SE_{\widehat{E[Y|x^*]}}\\
\widehat{E[Y|x^*]} &\pm t_{n-2}^* \cdot s_{residuals} \sqrt{\frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}
\end{align}

```{r}
#| echo: false
ggplot(gapm,
       aes(x=female_literacy_rate_2011, 
           y=life_expectancy_years_2011)) +
  geom_point()+
  geom_smooth(method = lm, se=TRUE)+
  ggtitle("Life expectancy vs. female literacy rate") +
  geom_vline(xintercept = mx, color = "purple")
```



## Prediction interval for predicting __individual__ observations

* We do not call this interval a CI since $Y$ is a random variable instead of a parameter
* The form is similar to a CI though:

$$\widehat{Y|x^*} \pm t_{n-2}^* \cdot s_{residuals} \sqrt{1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}$$

* Note that the only difference to the CI for a mean value of y is the additional `1+` under the square root.
    * Thus the width is wider!



### Example: Prediction interval

__Find the 95% prediction interval for the life expectancy when the female literacy rate is 60.__

\begin{align}
\widehat{Y|x^*} &\pm t_{n-2}^* \cdot s_{residuals} \sqrt{1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}\\
64.8596 &\pm 1.990847 \cdot 6.142157 \sqrt{1+\frac{1}{80} + \frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\
(52.48072 &, 77.23849)
\end{align}


```{r}
(Y60 <- 50.9278981 + 0.2321951 * 60)
(tstar <- qt(.975, df = 78))

(s_resid <- glance(model1)$sigma)

(n <- nobs(model1))
(mx <- mean(gapm$female_literacy_rate_2011))
(s_x <- sd(gapm$female_literacy_rate_2011))

(SE_Ypred <- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))

(MOE_Ypred <- SE_Ypred*tstar)

Y60 - MOE_Ypred
Y60 + MOE_Ypred
```



### Example: Using R for prediction interval

__Find the 95% prediction intervals for the life expectancy when the female literacy rate is 40, 60, and 80.__
    
```{r}
newdata  # previously defined for CI's

predict(model1, 
        newdata=newdata, 
        interval="prediction")  # prediction instead of "confidence"
```


### Interpretation

We are 95% confident that a new selected country with a 60% female literacy rate will have a life expectancy between 52.5 and 77.2 years.



## Prediction bands vs. confidence bands (1/2)

Create a scatterplot with the regression line, 95% confidence bands, and 95% prediction bands.


* First create a data frame with the original data points (both x and y values), their respective predicted values, andtheir respective prediction intervals
* Can do this with `augment()` from the `broom` package.

```{r}
model1_pred_bands <- augment(model1, interval = "prediction")

# take a look at new object:
names(model1_pred_bands) 

# glimpse of select variables of interest:
model1_pred_bands %>% 
  select(life_expectancy_years_2011, female_literacy_rate_2011, 
         .fitted:.upper) %>% 
  glimpse()
```


### Prediction bands vs. confidence bands (2/2)

```{r}
names(model1_pred_bands) 
```

```{r}
ggplot(model1_pred_bands, 
       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +
  geom_point() +
  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands
              alpha = 0.2, fill = "red") +
  geom_smooth(method=lm) +  # confidence bands
  labs(title = "SLR with Confidence & Prediction Bands") 

```


