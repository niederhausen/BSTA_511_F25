---
title: "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)"
subtitle: "BSTA 511/611"
author: "Meike Niederhausen, PhD"
institute: "OHSU-PSU School of Public Health"
date: "11/6/2024"
categories: ["Week 6"]
format: 
  html:
    link-external-newwindow: true
    toc: true
execute:
  echo: true
  freeze: auto  # re-render only when source changes
# editor: visual
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: "setup"
#| include: false
knitr::opts_chunk$set(echo = TRUE, fig.height=3, fig.width=5, message = F)
```

## Load packages

* Packages need to be loaded _every time_ you restart R or render an Qmd file

```{r}
# run these every time you open Rstudio
library(tidyverse)    
library(oibiostat)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) # NEW!!

set.seed(456)
```


- You can check whether a package has been loaded or not 
  - by looking at the Packages tab and 
  - seeing whether it has been checked off or not


# Goals for today

## Part 1 

### (4.3, 5.1) Hypothesis testing for mean from one sample  

* Introduce hypothesis testing using the case of analyzing a mean from one sample (group)

::: columns
::: {.column width="50%"}
* [Steps of a hypothesis test:]{style="color:purple"}
    1. level of significance
    1. null ( $H_0$ ) and alternative ( $H_A$ ) hypotheses
    1. test statistic
    1. p-value
    1. conclusion
:::
::: {.column width="50%"}
* [Run a hypothesis test in R]{style="color:green"}
    * Load a dataset - need to specify location of dataset
    * R projects
    * Run a t-test in R
    * `tidy()` the test output using `broom` package
:::
:::

### (4.3.3) Confidence intervals (CIs) vs. hypothesis tests 


## Part 2 - Class discussion

### (5.2) Inference for __mean difference__ from dependent/paired 2 samples  

* Inference: CIs and hypothesis testing
* Exploratory data analysis (EDA) to visualize data
* Run paired t-test in R

### One-sided CIs

### Class discussion

* Inference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.
* Thus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.
* In class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code. 


## MoRitz's tip of the day: use [__R projects__]{style="color:darkorange"} to organize analyses


MoRitz loves using R projects to 

* organize analyses and 
* make it easier to load data files 
* and also save output

Other bonuses include 

* making to it easier to collaborate with others, 
* including yourself when accessing files from different computers.

<br>


We will discuss how to use projects later in today's slides when loading a dataset.  
See file [Projects in RStudio](../resources/Projects_in_R.html) for more information.



# Is 98.6°F  really the mean "healthy" body temperature?

* __Where did the 98.6°F value come from?__
    * German physician Carl Reinhold August [Wunderlich](https://www.google.com/books/edition/_/a6UNq33GPfIC?hl=en&gbpv=1&pg=PP14) determined  98.6°F (or 37°C) based on temperatures from 25,000 patients in Leipzig in 1851.

* [1992 JAMA article](https://jamanetwork.com/journals/jama/article-abstract/400116) by Mackowiak, Wasserman, & Levine
    * They claim that 98.2°F (36.8°C) is a more accurate average body temp
    * Sample: n = 148 healthy men and women aged 18 - 40 years

* In January 2020, a group from Stanford published _[Decreasing human body temperature in the United States since the Industrial Revolution](https://elifesciences.org/articles/49555)_ in eLIFE.
    * "determined that mean body temperature in men and women, after adjusting for age, height, weight and, in some models date and time of day, has decreased monotonically by 0.03°C (0.05°F) per birth decade"
    * September 2023 update: _[Defining Usual Oral Temperature Ranges in Outpatients Using an Unsupervised Learning Algorithm](https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2809098)_ in JAMA Internal Medicine
        * Average is 36.64 °C (97.95 °F); "range of mean temperatures for the coolest to the warmest individuals was 36.24 °C to 36.89 °C" (97.23 to 98.40 °F); based 2008-2017 data
        * "findings suggest that age, sex, height, weight, and time of day are factors that contribute to variations in individualized normal temperature ranges."

* NYT article [The Average Human Body Temperature Is Not 98.6 Degrees](../resources/NYT_What_Is_a_Fever_Why_Your_Body_Temperature_May_Be_Cooler_Than_98.6_Degrees.pdf), Oct 12, 2023, by Dana G. Smith

__[Question:]{style="color:darkorange"} based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?__



## Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?


Two approaches to answer this question:

1. Create a __[confidence interval (CI)]{style="color:purple"}__ for the population mean $\mu$ and determine whether 98.6°F is inside the CI or not.
    * is 98.6°F a plausible value?

2. Run a __[hypothesis test]{style="color:green"}__ to see if there is evidence that the population mean $\mu$ is _significantly different_ from 98.6°F or not.
    * This does not give us a range of plausible values for the population mean $\mu$.
    
    * Instead, we calculate a _test statistic_ and _p-value_ 
        * to see how likely we are to observe the sample mean $\bar{x}$
        * or a more extreme sample mean 
        * assuming that the population mean $\mu$ is 98.6°F.



### Approach 1: Create a [95% C I]{style="color:purple"} for the population mean body temperature

* Use data based on the results from the 1992 JAMA study
    * The original dataset used in the JAMA article is not available
    * However, Allen Shoemaker from Calvin College created a [dataset](http://jse.amstat.org/datasets/normtemp.dat.txt) with the same summary statistics as in the JAMA article, which we will use:

$$\bar{x} = 98.25,~s=0.733,~n=130$$
CI for $\mu$:

```{r}
#| include: false
n <- 130
xbar <- 98.25
sd <- 0.733
(tstar <- qt(.975, df=n-1))  # df = n-1
(se <- sd/sqrt(n))
(moe <- tstar * se) 
(LB <- xbar - moe)
(UB <- xbar + moe)
```


\begin{align}
\bar{x} &\pm t^*\cdot\frac{s}{\sqrt{n}}\\
98.25 &\pm `r round(tstar,3)`\cdot\frac{0.733}{\sqrt{130}}\\
98.25 &\pm `r round(moe,3)`\\
(`r round(LB, 3)`&, `r round(UB, 3)`)
\end{align}

Used $t^*$ = `qt(.975, df=129)` 

Conclusion:  
We are 95% confident that the (population) mean body temperature is between `r round(LB, 3)`°F and `r round(UB, 3)`°F.

* _How does the CI compare to 98.6°F?_


### Approach 2: [Hypothesis Test]{style="color:green"} 

From before: 

* Run a __hypothesis test__ to see if there is evidence that the population mean $\mu$ is _significantly different_ from 98.6°F or not.
    * This does not give us a range of plausible values for the population mean $\mu$.
    
    * Instead, we calculate a _test statistic_ and _p-value_ 
        * to see how likely we are to observe the sample mean $\bar{x}$
        * or a more extreme sample mean 
        * assuming that the population mean $\mu$ is 98.6°F.

__How do we calculate a _test statistic_ and _p-value_?__




### Recall the sampling distribution of the mean

From the __[Central Limit Theorem (CLT)]{style="color:darkorange"}__, we know that

* For **["large" sample sizes]{style="color:cornflowerblue"}** ( $n\geq 30$ ),
    * the [__sampling distribution__ of the sample mean]{style="color:green"}
    * can be approximated by a __normal distribution__,with 
      * _mean_ equal to the _population mean_ value $\mu$, and 
      * _standard deviation_ $\frac{\sigma}{\sqrt{n}}$

$$\bar{X}\sim N\Big(\mu_{\bar{X}} = \mu, \sigma_{\bar{X}}= \frac{\sigma}{\sqrt{n}}\Big)$$

* For **[small sample sizes]{style="color:cornflowerblue"}**, if the population is known to be normally distributed, then
    * the same result holds



### Case 1: suppose we know the population sd $\sigma$

* [How likely we are to observe the sample mean $\bar{x}$ ,]{style="color:green"}
    * [or a more extreme sample mean, ]{style="color:green"}
    * [assuming that the population mean $\mu$ is 98.6°F?]{style="color:green"}
* Use $\bar{x} = 98.25$, $\sigma=0.733$, and $n=130$


```{r}
#| fig.width: 6
#| fig.height: 3
#| echo: false
# specify upper and lower bounds of shaded region below
mu <- 98.6
std <- 0.06428835

ggplot(data.frame(x = c(mu-4*std, mu+4*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  # stat_function(fun = dnorm, 
  #               args = list(mean = mu, sd = std), 
  #         # specify the upper and lower bounds of the shaded region:
  #               xlim = c(mu-4*std, xbar),             
  #               geom = "area", fill = "darkblue") +
  # the breaks values below might need to be adjusted 
  # if there are too many values showing on the x-axis
  # scale_x_continuous(breaks=(mu-4*std):(mu+4*std)) +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 0.06*(1:5), mu + 0.06*(1:5))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "sample mean",
       title = "Sampling distribution of mean body temperatures") 
```


### Case 2: we don't know the population sd $\sigma$ 


* This is usually the case in real life
* We estimate $\sigma$ with the sample standard deviation $s$
* From last time, we know that in this case we need to use the __t-distribution with d.f. = n-1__, instead of the normal distribution 
* [Question: How likely we are to observe the sample mean $\bar{x}$ or a more extreme sample mean, assuming that the population mean $\mu$ is 98.6°F?]{style="color:green"}
* Use $\bar{x} = 98.25$, $s=0.733$, and $n=130$


# Steps in a Hypothesis Test


1. Set the __[level of significance]{style="color:darkorange"}__ $\alpha$

1. Specify the __[null]{style="color:darkorange"}__ ( $H_0$ ) and __[alternative]{style="color:darkorange"}__ ( $H_A$ ) __[hypotheses]{style="color:darkorange"}__
    1. In symbols
    1. In words
    1. Alternative: one- or two-sided?

1. Calculate the __[test statistic]{style="color:darkorange"}__. 

1. Calculate the __[p-value]{style="color:darkorange"}__ based on the observed test statistic and its sampling distribution

1. Write a __[conclusion]{style="color:darkorange"}__ to the hypothesis test
    1. Do we reject or fail to reject $H_0$?
    1. Write a conclusion in the context of the problem



## Step 2: Null & Alternative Hypotheses

In statistics, a __hypothesis__ is a statement about the value of an __unknown population parameter__.


A __[hypothesis test]{style="color:darkorange"}__ consists of a test between two competing hypotheses: 

1. a __[null]{style="color:darkorange"}__ hypothesis $H_0$ (pronounced “H-naught”) vs. 
1. an __[alternative]{style="color:darkorange"}__ hypothesis $H_A$ (also denoted $H_1$)

Example of hypotheses in words: 

\begin{aligned}
H_0 &: \text{The population mean body temperature is 98.6°F}\\
\text{vs. } H_A &: \text{The population mean body temperature is not 98.6°F}
\end{aligned}

1. $H_0$ is a claim that there is “no effect” or “no difference of interest.”
1. $H_A$ is the claim a researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis $H_0$

### Notation for hypotheses

\begin{aligned}
H_0 &: \mu = \mu_0\\
\text{vs. } H_A&: \mu \neq, <, \textrm{or}, > \mu_0
\end{aligned}

We call $\mu_0$ the *null value*

::: columns
::: {.column width="40%"}
$H_A: \mu \neq \mu_0$

::: {style="font-size: 90%;"}
* not choosing a priori whether we believe the population mean is greater or less than the null value $\mu_0$
:::
:::

::: {.column width="30%"}
$H_A: \mu < \mu_0$

::: {style="font-size: 90%;"}
* believe the population mean is **less** than the null value $\mu_0$
:::
:::

::: {.column width="30%"}
$H_A: \mu > \mu_0$

::: {style="font-size: 90%;"}
* believe the population mean is **greater** than the null value $\mu_0$
:::
:::
:::

* $H_A: \mu \neq \mu_0$ is the most common option, since it's the most conservative

Example: 

\begin{aligned}
H_0 &: \mu = 98.6\\
\text{vs. } H_A&: \mu \neq 98.6
\end{aligned}



## Step 3: [Test statistic]{style="color:darkorange"} (& its distribution)

__[Case 1: know population sd $\sigma$]{style="color:purple"}__

$$
\text{test statistic} = z_{\bar{x}} = \frac{\bar{x} - \mu_0}{\frac{\sigma}{\sqrt{n}}}
$$

* Statistical theory tells us that [$z_{\bar{x}}$]{style="color:purple"} follows a [__Standard Normal distribution__ $N(0,1)$]{style="color:purple"}


__[Case 2: don't know population sd $\sigma$]{style="color:green"}__

$$
\text{test statistic} = t_{\bar{x}} = \frac{\bar{x} - \mu_0}{\frac{s}{\sqrt{n}}}
$$

* Statistical theory tells us that [$t_{\bar{x}}$]{style="color:green"} follows a [__Student's t distribution__ with degrees of freedom (df) = $n-1$]{style="color:green"}



$\bar{x}$ = sample mean,  
$\mu_0$ = hypothesized population mean from $H_0$,  
$\sigma$ = _population_ standard deviation,  
$s$ = _sample_ standard deviation,   
$n$ = sample size

__[Assumptions]{style="color:darkorange"}__: same as CLT

* __Independent observations__ 
    * The observations were collected independently.
* __Approximately normal sample or big n__ 
    * The distribution of the sample should be approximately normal 
    * _or_ the sample size should be at least 30.

## Step 3: Test statistic calculation

Recall that $\bar{x} = 98.25$, $s=0.733$, and $n=130.$

The test statistic is:

$$t_{\bar{x}} = \frac{\bar{x} - \mu_0}{\frac{s}{\sqrt{n}}}
= \frac{98.25 - 98.6}{\frac{0.73}{\sqrt{130}}}
= -5.45$$

* Statistical theory tells us that $t_{\bar{x}}$ follows a __Student's t-distribution__ with $d.f. = n-1 = 129$.


```{r}
#| fig.width: 6
#| fig.height: 3
#| echo: false
ggplot(data = data.frame(x = c(-6, 6)), aes(x)) + 
  stat_function(fun = dt, args = list(df = 129)) + 
  ylab("") + 
  xlab("t-dist with df = 129") +
  scale_y_continuous(breaks = NULL) + 
  geom_vline(xintercept = c(-5.45,5.45), 
             color = "red")
```


__Assumptions met?__

## Step 4: p-value

The __[p-value]{style="color:darkorange"}__ is the [__probability__ of obtaining a test statistic _just as extreme or more extreme_ than the observed test statistic assuming the null hypothesis $H_0$ is true.]{style="color:darkblue"} 


* The $p$-value is a quantification of "surprise"
    * Assuming $H_0$ is true, _how surprised are we with the observed results_?
    * _Ex_: assuming that the true mean body temperature is 98.6°F, how surprised are we to get a sample mean of 98.25°F  (or more extreme)?
    
* If the $p$-value is "small," it means there's a small probability that we would get the observed statistic (or more extreme) when $H_0$ is true.



```{r}
#| fig.width: 6
#| fig.height: 3
#| echo: false
# specify upper and lower bounds of shaded region below
mu <- 98.6
std <- 0.06428835

ggplot(data.frame(x = c(mu-4*std, mu+4*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  # stat_function(fun = dnorm, 
  #               args = list(mean = mu, sd = std), 
  #         # specify the upper and lower bounds of the shaded region:
  #               xlim = c(mu-4*std, xbar),             
  #               geom = "area", fill = "darkblue") +
  # the breaks values below might need to be adjusted 
  # if there are too many values showing on the x-axis
  # scale_x_continuous(breaks=(mu-4*std):(mu+4*std)) +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 0.06*(1:5), mu + 0.06*(1:5))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "sample mean",
       title = "Sampling distribution of mean body temperatures") 
```


## Step 4: p-value calculation 

Calculate the _p_-value using the __Student's t-distribution__ with $d.f. = n-1 = 129$:

$$p-value=P(T \leq -5.45) + P(T \geq 5.45) = 2.410889 \times 10^{-07}$$

```{r}
# use pt() instead of pnorm()
# need to specify df
2*pt(-5.4548, df = 130-1, lower.tail = TRUE)
```  

```{r}
#| fig.width: 6
#| fig.height: 3
#| echo: false
ggplot(data = data.frame(x = c(-6, 6)), aes(x)) + 
  stat_function(fun = dt, args = list(df = 129)) + 
  ylab("") + 
  xlab("t-dist with df = 129") +
  scale_y_continuous(breaks = NULL) + 
  geom_vline(xintercept = c(-5.45,5.45), 
             color = "red")
```

## Step 4: p-value estimation using $t$-table

* $t$-table only gives us *bounds* on the p-value
* Recall from using the $t$-table for CIs, that the table gives us the cutoff values for varying tail probabilities (1-tail & 2-tail)
* Find the row with the appropriate degrees of freedom
    * Use next smallest df in table if actual df not shown
    * I.e., for df = 129, use df = 100 in table
* Figure out where the test statistic's absolute value is in relation to the values in the columns, i.e. between which columns is the test statistic?
* The header rows for those columns gives the lower & upper bounds for the p-value
    * Choosing one-tail vs. two-tail test, depends on the alternative hypothesis $H_A$.
    * For a 2-sided test ( $H_A: \mu \neq \mu_0$ ), use two-tails 
    * For a 1-sided test ( $H_A: \mu < \textrm{or} > \mu_0$ ), use one-tail 
    

## Step 1: Significance Level $\alpha$

* __Before doing a hypothesis test__, we set a cut-off for how small the $p$-value should be in order to reject $H_0$.
* We call this the __[significance level]{style="color:darkorange"}__, denoted by the Greek symbol [alpha ( $\alpha$ )]{style="color:darkorange"}
* [Typical $\alpha$]{style="color:darkblue"} values are 
    * 0.05 - _most common by far!!_
    * 0.01 and 0.1
* Decision rule:    
    * When [$p$-value < $\alpha$]{style="color:green"}, we "__[reject]{style="color:green"}__ the null hypothesis [$H_0$]{style="color:green"}."
    * When [$p$-value $\geq \alpha$]{style="color:purple"}, we "__[fail to reject]{style="color:purple"}__ the null hypothesis [$H_0$]{style="color:purple"}."

:::{.callout-important}
* "Failing to reject" $H_0$ is __NOT__ the same as "accepting" $H_0$! 
* By failing to reject $H_0$ we are just saying that we don't have sufficient evidence to support the alternative $H_A$.
* _This does not imply that $H_0$ is true!!_
:::



## Step 5: Conclusion to hypothesis test

\begin{aligned}
H_0 &: \mu = 98.6\\
\text{vs. } H_A&: \mu \neq 98.6
\end{aligned}

* Recall the $p$-value = $2.410889 \times 10^{-07}$ 
* Use $\alpha$ = 0.05.
* Do we reject or fail to reject $H_0$?

__Conclusion statement__:

* Basic: ("stats class" conclusion)
    * There is sufficient evidence that the (population) mean body temperature is discernibly different from 98.6°F ( $p$-value < 0.001).

* Better: ("manuscript style" conclusion)
    * The average body temperature in the sample was 98.25°F (95% CI 98.12, 98.38°F), which is discernibly different from 98.6°F ( $p$-value < 0.001).

## Confidence Intervals vs. Hypothesis Testing

* See also V&H Section 4.3.3

# Running a t-test in R

* Working directory
* Load a dataset - need to specify location of dataset
* R projects
* Run a t-test in R
* `tidy()` the test output using `broom` package

## Working directory

* In order to load a dataset from a file, you need to tell R where the dataset is located
* To do this you also need to know the location from which R is working, i.e. your __working directory__
* You can figure out your working directory by running the `getwd()` function.

```{r}
getwd()
```

* Above is the working directory of this slides file
    * _In this case, this is NOT the location of the actual qmd file though!_

* To make it easier to juggle the working directory, the location of your qmd file, and the location of the data, 
    * [I highly recommend using __R Projects__!]{style="color:darkred"}

## R projects

* I *highly, highly, HIGHLY* recommend using R Projects to organize your analyses and make it easier to load data files and also save output.
* When you create an R Project on your computer, the Project is associated with the folder (directory) you created it in. 
  * This folder becomes the "root" of your working directory, and RStudio's point of reference from where to load files from and to. 
* I create separate Projects for every analysis project and every class I teach. 
* You can run multiple sessions of RStudio by opening different Projects, and the environments (or working directory) of each are working independently of each other. 

:::{.callout-note}
* Although we are using Quarto files, 
    * I will show how to set up and use a __"regular" R Project__
    * instead of "Quarto Project"
* Quarto Projects include extra features and thus complexity. Once you are used to how regular R Projects work, you can try out a Quarto Project. 
:::

## How to create an R Project

* Demonstration in class recording
* Posit's (RStudio's) directions for creating Projects
  * [https://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects)

* See file [Projects in RStudio](../slides_code/Projects_in_R.html) for more information on R Projects.


## Load the dataset

* The data are in a csv file called `BodyTemperatures.csv`
* You need to tell R where the dataset is located!
* I recommend saving all datasets in a folder called data. 
    * The code I will be providing you will be set up this way.

* To make it easier to specify where the dataset is located, I recommend using the `here()` function from the `here` package: `here::here()`.

```{r}
# read_csv() is a function from the readr package that is a part of the tidyverse
library(here)   # first install this package

BodyTemps <- read_csv(here::here("data", "BodyTemperatures.csv"))
#                     location: look in "data" folder
#                               for the file "BodyTemperatures.csv"

glimpse(BodyTemps)
```

## `here::here()`

General use of `here::here()`

`here::here("folder_name", "filename")`

Resources for `here::here()`:

* [how to use the `here` package](http://jenrichmond.rbind.io/post/how-to-use-the-here-package/) (Jenny Richmond)
* [Ode to the here package](https://github.com/jennybc/here_here) (Jenny Bryan)

[Project-oriented workflow](https://www.tidyverse.org/blog/2017/12/workflow-vs-script/) (Jenny Bryan)



## `t.test`: base R's function for testing one mean 

* Use the body temperature example with $H_A: \mu \neq 98.6$
* We called the dataset `BodyTemps` when we loaded it

```{r}
glimpse(BodyTemps)

(temps_ttest <- t.test(x = BodyTemps$Temperature,
       # alternative = "two.sided",  # default
       mu = 98.6))
```

Note that the test output also gives the 95% CI using the t-distribution.


## `tidy()` the `t.test` output

* Use the `tidy()` function from the `broom` package for briefer output in table format that's stored as a `tibble`
* Combined with the `gt()` function from the `gt` package, we get a nice table 

```{r}
tidy(temps_ttest) %>% 
  gt()
```

* Since the `tidy()` output is a tibble, we can easily `pull()` specific values from it:

::: columns
::: {.column width="50%"}

Using base R's `$`
```{r}
tidy(temps_ttest)$p.value  
```

Advantage: quick and easy
:::

::: {.column width="50%"}
Or the `tidyverse` way: using `pull()` from `dplyr` package 
```{r}
tidy(temps_ttest) %>% pull(p.value)
```

Advantage: can use together with piping (`%>%`) other functions
:::
:::


## What's next? 

CI's and hypothesis testing for different scenarios:


| Day | Section |  Population parameter   |       Symbol        |       Point estimate       |            Symbol             |
|:----:|:------:|:----------:|:--------:|:----------:|:-------:|
| 10  |   5.1   |        Pop mean         |        $\mu$        |        Sample mean         |           $\bar{x}$           |
| 10  |   5.2   | Pop mean of paired diff | $\mu_d$ or $\delta$ | Sample mean of paired diff |         $\bar{x}_{d}$         |
| 11  |   5.3   |    Diff in pop means    |    $\mu_1-\mu_2$    |    Diff in sample means    |    $\bar{x}_1 - \bar{x}_2$    |
| 12  |   8.1   |     Pop proportion      |         $p$         |        Sample prop         |         $\widehat{p}$         |
| 12  |   8.2   |   Diff in pop prop's    |      $p_1-p_2$      |   Diff in sample prop's    | $\widehat{p}_1-\widehat{p}_2$ |



# Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data  (Section 5.2)

## What we covered in Day 10 Part 1 

### (4.3, 5.1) Hypothesis testing for mean from one sample  

* Introduce hypothesis testing using the case of analyzing a mean from one sample (group)

::: columns
::: {.column width="50%"}
* [Steps of a hypothesis test:]{style="color:purple"}
    1. level of significance
    1. null ( $H_0$ ) and alternative ( $H_A$ ) hypotheses
    1. test statistic
    1. p-value
    1. conclusion
:::
::: {.column width="50%"}
* [Run a hypothesis test in R]{style="color:green"}
    * Load a dataset - need to specify location of dataset
    * R projects
    * Run a t-test in R
    * `tidy()` the test output using `broom` package
:::
:::

### (4.3.3) Confidence intervals (CIs) vs. hypothesis tests 


## Goals for today: Part 2 - Class discussion

### (5.2) Inference for __mean difference__ from dependent/paired 2 samples  

* Inference: CIs and hypothesis testing
* Exploratory data analysis (EDA) to visualize data
* Run paired t-test in R

### One-sided CIs

### Class discussion

* Inference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.
* Thus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.
* In class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code. 


## CI's and hypothesis tests for different scenarios: 

::: {style="font-size: 90%;"}
$$\text{point~estimate} \pm z^*(or~t^*)\cdot SE,~~\text{test~stat} = \frac{\text{point~estimate}-\text{null~value}}{SE}$$

Day | Book | Population <br> parameter | Symbol | Point estimate | Symbol | SE
--|--|--|--|--|--|--
10 | 5.1 | Pop mean | $\mu$ | Sample mean | $\bar{x}$ | $\frac{s}{\sqrt{n}}$
10 | 5.2 | [Pop mean of paired diff]{style="color:green"} | [$\mu_d$ or $\delta$]{style="color:green"} | [Sample mean of paired diff]{style="color:green"} | [$\bar{x}_{d}$]{style="color:green"}  | [**???**]{style="color:red"}
11 | 5.3 | Diff in pop <br> means | $\mu_1-\mu_2$ | Diff in sample <br> means | $\bar{x}_1 - \bar{x}_2$  |
12 | 8.1 | Pop proportion | $p$ | Sample prop | $\widehat{p}$  |
12 | 8.2 | Diff in pop <br> proportions | $p_1-p_2$ | Diff in sample <br> proportions | $\widehat{p}_1-\widehat{p}_2$ |

:::


## Steps in a Hypothesis Test


1. Set the __[level of significance]{style="color:darkorange"}__ $\alpha$

1. Specify the __[null]{style="color:darkorange"}__ ( $H_0$ ) and __[alternative]{style="color:darkorange"}__ ( $H_A$ ) __[hypotheses]{style="color:darkorange"}__
    1. In symbols
    1. In words
    1. Alternative: one- or two-sided?

1. Calculate the __[test statistic]{style="color:darkorange"}__. 

1. Calculate the __[p-value]{style="color:darkorange"}__ based on the observed test statistic and its sampling distribution

1. Write a __[conclusion]{style="color:darkorange"}__ to the hypothesis test
    1. Do we reject or fail to reject $H_0$?
    1. Write a conclusion in the context of the problem


# Goals for today (Section 5.2)

* Statistical inference for paired data (2 samples)
    1. What are paired data?  
    1. EDA of data
    1. Hypothesis test
    1. Confidence Interval
    1. Run test in R


## Examples of paired designs (two samples)

* Enroll pairs of identical twins to study a disease
* Enroll father & son pairs to study cholesterol levels
* Studying pairs of eyes
* Enroll people and collect data before & after an intervention (longitudinal data)
* Book: Compare maximal speed of competitive swimmers wearing a wetsuit vs. wearing a regular swimsuit


# Can a vegetarian diet change cholesterol levels?

* __Scenario__:
  * 24 non-vegetarian people were enrolled in a study
  * They were instructed to adopt a vegetarian diet
  * Cholesterol levels were measured before and after the diet
* __Question__: Is there evidence to support that cholesterol levels changed after the vegetarian diet?
* How to answer the question?
  * First, calculate changes (differences) in cholesterol levels
      * We usually do after - before if the data are longitudinal


Calculate __CI for the mean difference__ $\delta$:

$$\bar{x}_d \pm t^*\cdot\frac{s_d}{\sqrt{n}}$$

Run a __hypothesis test__

Hypotheses

$$H_0: \delta = \delta_0 \\
H_A: \delta \neq \delta_0 \\
(or~ <, >)$$


Test statistic

$$
t_{\bar{x}_d} = \frac{\bar{x}_d - \delta_0}{\frac{s_d}{\sqrt{n}}}
$$


## EDA: Explore the cholesterol data

* Scenario:
  * 24 non-vegetarian people were enrolled in a study
  * They were instructed to adopt a vegetarian diet
  * Cholesterol levels were measured before and after the diet

```{r}
chol <- read_csv("data/chol213.csv")
glimpse(chol)
chol %>% get_summary_stats(type = "common") %>% gt()
```


### EDA: Cholesterol levels before and after vegetarian diet


```{r }
ggplot(chol, aes(x=Before)) +
  geom_density()
ggplot(chol, aes(x=Before)) +
  geom_boxplot()
```

```{r }
ggplot(chol, aes(x=After)) +
  geom_density()
ggplot(chol, aes(x=After)) +
  geom_boxplot()
```


### EDA: Spaghetti plot of cholesterol levels before & after diet

* Visualize the individual before vs. after diet changes in cholesterol levels

```{r}
#| fig.height: 4
chol_long <- chol %>% 
  # need an ID column for the plot
  # make it factor so that coloring is not on continuous scale
  mutate(ID = factor(1:n())) %>% 
  # make data long for plot: 
  pivot_longer(
    cols = Before:After,
    names_to = "Time",  # need a column for Before & After on x-axis
    values_to = "Cholesterol") %>% # need a column of all cholesterol values for y-axis
  mutate(
    # change Time a factor variable so that can reorder
    # levels so that Before is before After
    Time = factor(Time, levels = c("Before", "After"))
    )
  
ggplot(chol_long, 
       aes(x=Time, y = Cholesterol, 
           # need to include group = ID 
           # to create a line for each ID
           color = ID, group = ID)) + 
  geom_line(show.legend = FALSE)
```


### EDA: Differences in cholesterol levels: After - Before diet


```{r }
chol <- chol %>% 
  mutate(DiffChol = After-Before) 
head(chol, 12)

```

```{r }
chol %>% 
  get_summary_stats(type = "common") %>% 
  gt()
```


### EDA: Differences in cholesterol levels: After - Before diet

```{r }
ggplot(chol, aes(x=Before)) +
  geom_density()
ggplot(chol, aes(x=After)) +
  geom_density()
```

```{r }
ggplot(chol, aes(x=DiffChol)) + 
  geom_density()
ggplot(chol, aes(x=DiffChol)) + 
  geom_boxplot()
```


# Steps in a Hypothesis Test


1. Set the __[level of significance]{style="color:darkorange"}__ $\alpha$

1. Specify the __[null]{style="color:darkorange"}__ ( $H_0$ ) and __[alternative]{style="color:darkorange"}__ ( $H_A$ ) __[hypotheses]{style="color:darkorange"}__
    1. In symbols
    1. In words
    1. Alternative: one- or two-sided?

1. Calculate the __[test statistic]{style="color:darkorange"}__. 

1. Calculate the __[p-value]{style="color:darkorange"}__ based on the observed test statistic and its sampling distribution

1. Write a __[conclusion]{style="color:darkorange"}__ to the hypothesis test
    1. Do we reject or fail to reject $H_0$?
    1. Write a conclusion in the context of the problem


## Step 2: Null & Alternative Hypotheses 

* __Question__: Is there evidence to support that cholesterol levels changed after the vegetarian diet?


Null and alternative hypotheses in __words__
Include as much context as possible

<br>

* $H_0$: The population mean difference in cholesterol levels after a vegetarian diet is  

* $H_A$: The population mean difference in cholesterol levels after a vegetarian diet is



Null and alternative hypotheses in __symbols__

$$~~~~H_0: \delta =  \\
H_A: \delta   \\$$


## Step 3: Test statistic 

```{r}
chol %>% select(DiffChol) %>% get_summary_stats(type = "common") %>% gt()
```

$$
t_{\bar{x}_d} = \frac{\bar{x}_d - \delta_0}{\frac{s_d}{\sqrt{n}}}
$$

* Based on the value of the test statistic, do you think we are going to reject or fail to reject $H_0$?
* What probability distribution does the test statistic have?
* Are the __[assumptions]{style="color:darkorange"}__ for a paired t-test satisfied so that we can use the probability distribution to calculate the $p$-value??


```{r}
n <- 24
alpha <- 0.05
mu <- 0
(p_area <- 1-alpha/2)
(xbar <- mean(chol$DiffChol))
(sd <- sd(chol$DiffChol))
(se <- sd/sqrt(n))

(tstat <- (xbar - mu)/se)
```



## Step 4: p-value

The __[p-value]{style="color:darkorange"}__ is the __probability__ of obtaining a test statistic _just as extreme or more extreme_ than the observed test statistic assuming the null hypothesis $H_0$ is true. 


```{r}
# specify upper and lower bounds of shaded region below
mu <- 0
std <- se

# The following figure is only an approximation of the 
# sampling distribution since I used a normal instead
# of t-distribution to make it.

ggplot(data.frame(x = c(mu-6*std, mu+6*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 3.4*(1:6), mu + 3.4*(1:6))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "sample mean difference",
       title = "Sampling distribution of mean difference") +
  geom_vline(xintercept = c(-xbar, xbar), 
             color = "red")
```

```{r}
ggplot(data = data.frame(x = c(-6, 6)), aes(x)) + 
  stat_function(fun = dt, args = list(df = n-1)) + 
  ylab("") + 
  xlab("t-dist with df = 23") +
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks=c(mu, mu - (1:5), mu + (1:5))) +
  geom_vline(xintercept = c(-tstat,tstat), 
             color = "red")
```


Calculate the _p_-value:

```{r}
(pv <- 2*(pt(tstat, df=n-1)))
```  


## Step 5: Conclusion to hypothesis test

$$~~~~H_0: \delta = 0 \\
H_A: \delta \neq 0  \\$$

* Recall the $p$-value = $8.434775 \cdot 10 ^{-6}$
* Use $\alpha$ = 0.05.
* Do we reject or fail to reject $H_0$?

__Conclusion statement__:

* Stats class conclusion
    * There is sufficient evidence that the (population) mean difference in cholesterol levels after a vegetarian diet is different from 0 mg/dL ( $p$-value < 0.001).

* More realistic manuscript conclusion: 
    * After a vegetarian diet, cholesterol levels decreased by on average 19.54 mg/dL (SE = 3.43 mg/dL, 2-sided $p$-value < 0.001).



# 95% CI for the mean difference in cholesterol levels

```{r}
chol %>% select(DiffChol) %>% get_summary_stats(type = "common") %>% gt()
```

CI for $\mu_d$ (or $\delta$):

```{r}
n <- 24
alpha <- 0.05
(p_area <- 1-alpha/2)
(xbar <- mean(chol$DiffChol))
(sd <- sd(chol$DiffChol))
(tstar <- qt(p_area, df=n-1))  # df = n-1
(se <- sd/sqrt(n))
(moe <- tstar * se) 
(LB <- xbar - moe)
(UB <- xbar + moe)
```


$$\bar{x}_d \pm t^*\cdot\frac{s_d}{\sqrt{n}}\\
`r round(xbar,3)` \pm `r round(tstar,3)`\cdot\frac{`r round(sd,3)`}{\sqrt{`r n`}}\\
`r round(xbar,3)` \pm `r round(tstar,3)`\cdot `r round(se,3)`\\
`r round(xbar,3)` \pm `r round(moe,3)`\\
(`r round(LB, 3)`, `r round(UB, 3)`)$$


Conclusion:  
We are 95% that the (population) mean difference in cholesterol levels after a vegetarian diet is between `r round(LB, 3)` mg/dL and `r round(UB, 3)` mg/dL.

* _Based on the CI, is there evidence the diet made a difference in cholesterol levels? Why or why not?_

# R

## R option 1: Run a 1-sample `t.test` using the paired differences

$H_A: \delta \neq 0$

```{r}
t.test(x = chol$DiffChol, mu = 0)
```



## R option 2: Run a 2-sample `t.test` with `paired = TRUE` option

$H_A: \delta \neq 0$

* For a 2-sample t-test we specify both `x=` and `y=`
* Note: `mu = 0` is the default value and doesn't need to be specified

```{r}
t.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE)
```



## R option 3: Run a 2-sample `t.test` with `paired = TRUE` option, but using the long data and a "formula"


* Use the usual `t.test`
* What's different is that 
    * instead of specifying the variables with `x=` and `y=`, 
    * we give a __formula__ of the form `y ~ x` using _just the variable names_,
    * and then specify the name of the dataset using `data =`
* This method is often used in practice, and more similar to the coding style of running a regression model (BSTA 512 & 513)



```{r}
# using long data with columns Cholesterol & Time:
t.test(Cholesterol ~ Time, 
       paired = TRUE, 
       data = chol_long)
```



## `tidy` the `t.test` output & compare the 3 options


```{r}
# option 1
t.test(x = chol$DiffChol, mu = 0) %>% tidy() %>% gt() # tidy from broom package
# option 2
t.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE) %>% tidy() %>% gt()
# option 3
t.test(Cholesterol ~ Time, paired = TRUE, data = chol_long) %>% tidy() %>% gt()
```



# What if we wanted to test whether the diet _decreased_ cholesterol levels?


How are the steps different?

1. Set the __[level of significance]{style="color:darkorange"}__ $\alpha$

1. Specify the __[hypotheses]{style="color:darkorange"}__ $H_0$ and $H_A$
    * Alternative: one- or two-sided?

1. Calculate the __[test statistic]{style="color:darkorange"}__. 

1. Calculate the __[p-value]{style="color:darkorange"}__ based on the observed test statistic and its sampling distribution

1. Write a __[conclusion]{style="color:darkorange"}__ to the hypothesis test


## R: What if we wanted to test whether the diet _decreased_ cholesterol levels?

```{r}
# alternative = c("two.sided", "less", "greater")
t.test(x = chol$DiffChol, mu = 0, alternative = "less") %>% tidy() %>% gt()
```



# One-sided confidence intervals


Formula for a __2-sided__ (1- $\alpha$ )% __CI__:

$$\bar{x} \pm t^*\cdot\frac{s}{\sqrt{n}}$$
* $t^*$ = `qt(1-alpha/2, df = n-1)`
* $\alpha$ is split over both tails of the distribution


A __one-sided__ (1- $\alpha$ )% __CI__ has all (1- $\alpha$ )% on just the left or the right tail of the distribution:

$$(\bar{x} - t^*\cdot\frac{s}{\sqrt{n}},~\infty) \\
(\infty,~\bar{x} + t^*\cdot\frac{s}{\sqrt{n}})$$

* $t^*$ = `qt(1-alpha, df = n-1)` for a 1-sided lower (1- $\alpha$ )% CI
* $t^*$ = `qt(alpha, df = n-1)` for a 1-sided upper (1- $\alpha$ )% CI
* A 1-sided CI gives estimates for a lower or upper bound of the population mean.
* See Section 4.2.3 of the V&H book for more


# Confidence Intervals vs. Hypothesis Testing

* See also V&H Section 4.3.3



# What's next? 


CI's and hypothesis tests for different scenarios:

$$point~estimate \pm z^*(or~t^*)\cdot SE,~~~~~~~~test~stat = \frac{point~estimate-null~value}{SE}$$

Day | Book | Population <br> parameter | Symbol | Point estimate | Symbol | SE
--------|--------|--------|--------|--------|--------|--------
11-12 | 5.1 | Pop mean | $\mu$ | Sample mean | $\bar{x}$ | $\frac{s}{\sqrt{n}}$
13 | 5.2 | Diff in paired <br> pop means | $\mu_d$ or $\delta$ | Diff in paired <br> sample means | $\bar{x}_{d}$  | $\frac{s_d}{\sqrt{n}}$
14 | 5.3 | Diff in pop <br> means | $\mu_1-\mu_2$ | Diff in sample <br> means | $\bar{x}_1 - \bar{x}_2$  |
 | 8.1 | Pop proportion | $p$ | Sample prop | $\widehat{p}$  |
 | 8.2 | Diff in pop <br> proportions | $p_1-p_2$ | Diff in sample <br> proportions | $\widehat{p}_1-\widehat{p}_2$ |

